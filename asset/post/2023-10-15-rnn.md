---
title: "What are Recurrent Neural Networks (RNN)?"
categories: NLP
---

## ðŸ¤” What is a RNN?

---

## ðŸ”„ A brief History

- Sharing parameters, which is the base of RNN, was devised in early 1980s.
- Without parameter sharing, we could not **generalize across sequences of different lenghts** & _across different positions in time_.
- Such sharing of information is particularly important when a specific piece of information can occur at multiple _positions_ in the input text. For eg:
  - _"I went to Japan in **2010**"_
  - _"In **2010**, I went to Japan"_
- The idea of RNN is very similar to TDNN (time-delay neural networks).
  - **TDNN**:
    - Uses convolution for _1D temporal sequence_.
    - Uses the same _convolution-kernel_ across time.
    - Each member of the output is a function of small number of neighboring members in the input sequence
  - **RNN**:
    - Uses the same _kernel_ across time.
    - Each member of the output is a function of all **previous** members in the input sequence
- RNN can be applied to _2D spatial data_ (eg. images) as well as _2D sequence data_ (eg. videos)
- Learning a _single shared model_ allows generalization to sequence lengths that did not appear in training set, and allows the model to be estimated with far fewer training examples than would be required for a model _without_ parameter sharing.
- `tanh`(hyperbolic tangent) activation function is traditionally used in RNN (owing to historical singificance too)
- $a^{(t)} = b + W\cdot h^{(t-1)} + U\cdot x^{(t)}$
